{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b8483bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a1. Beta estimated:\n",
      "\tb0=-11.540478115172002\n",
      "\tb1=0.005647107969070656\n",
      "\tb2=2.080919845838359e-05\n",
      "\n",
      "a2. Maximum likelihood achieved:\n",
      "\t0.9240876860567856\n",
      "\n",
      "b1. Smallest binary cross-entropy achieved:\n",
      "\t0.07895077026546393\n",
      "\n",
      "b2. Associated weights:\n",
      "\tb0=-6.084566255470831\n",
      "\tb1=2.7101693561602627\n",
      "\tb2=0.27452845779446855\n",
      "\n",
      "b3. Corresponding beta for non-standardized features:\n",
      "\tb0=-11.455223585577933\n",
      "\tb1=0.005603102979357885\n",
      "\tb2=2.0585559349906284e-05\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Tianyi Lu, UNI:tl3126, E-mail:tl3126@columbia.edu\n",
    "ACTU PS5841 Data Science Assignment 6\n",
    "'''\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Part A\n",
    "## Regrression\n",
    "df=pd.read_csv(\"~/Documents/Default.csv\")\n",
    "y=df.iloc[:,1]\n",
    "x=df.iloc[:,2:]\n",
    "reg=LogisticRegression(random_state=0,solver='lbfgs',multi_class='ovr')\n",
    "reg.fit(x,y)\n",
    "beta_reg=np.zeros((x.shape[1]+1,1))\n",
    "print(\"a1. Beta estimated:\\n\\tb0=\"+str(reg.intercept_[0]))\n",
    "beta_reg[0]=reg.intercept_[0]\n",
    "for i in range(reg.coef_.shape[1]):\n",
    "    print(\"\\tb\"+str(i+1)+\"=\"+str(reg.coef_[0][i]))\n",
    "    beta_reg[i+1]=reg.coef_[0][i]\n",
    "\n",
    "## Calculate maximum likelihood achieved\n",
    "n=x.shape[0]\n",
    "x_=np.copy(x)\n",
    "x_=np.hstack((np.ones((n,1)),x_))\n",
    "y_=np.copy(1*(y==\"Yes\")).reshape(-1,1)\n",
    "def activation(input):\n",
    "    return 1/(1+np.exp(-input))\n",
    "def loss(input,x,y=y_):\n",
    "    predict=activation(x@input)\n",
    "    output=np.transpose(1-y)@np.log(1-predict)+np.transpose(y)@np.log(predict)\n",
    "    return -output[0][0]/n\n",
    "print(\"\\na2. Maximum likelihood achieved:\\n\\t\"+str(np.exp(-loss(beta_reg,x_))))\n",
    "\n",
    "\n",
    "# Part B\n",
    "## Parameters\n",
    "rate=0.044 # Learning rate, a float\n",
    "update=200 # Number of iterations, a positive integer\n",
    "beta=np.array([[0.0],[0.0],[0.0]]).reshape(-1,1) # Initial beta, n+1 floats\n",
    "d=0.00001 # Step size used in calculation of gradient, a small positive float\n",
    "\n",
    "## Standardize x\n",
    "x_std=np.copy(x)\n",
    "for i in range(x.shape[1]):\n",
    "    x_std[:,i]=x_std[:,i]-np.mean(x_std[:,i])\n",
    "    x_std[:,i]=x_std[:,i]/np.std(x_std[:,i])\n",
    "x_std=np.hstack((np.ones((n,1)),x_std))\n",
    "\n",
    "## Define functions\n",
    "def grad(input):\n",
    "    output=np.zeros((x.shape[1]+1,1))\n",
    "    input_=np.copy(input)\n",
    "    for i in range(x.shape[1]+1):\n",
    "        input_[i,0]+=d\n",
    "        output[i,0]=(loss(input_,x_std)-loss(input,x_std))/d\n",
    "        input_[i,0]-=d\n",
    "    scale=np.sqrt(rate**2/np.sum(output**2))\n",
    "    output*=scale\n",
    "    return output\n",
    "class optimal:\n",
    "    def __init__(self,beta):\n",
    "        self.beta=np.copy(beta)\n",
    "        self.loss=loss(beta,x_std)\n",
    "    def update(self,beta,score):\n",
    "        self.beta=np.copy(beta)\n",
    "        self.loss=score\n",
    "best=optimal(beta)\n",
    "\n",
    "## Main\n",
    "for i in range(update):\n",
    "    beta-=grad(beta)\n",
    "    score=loss(beta,x_std)\n",
    "    if(score<best.loss):\n",
    "        best.update(beta,score)\n",
    "\n",
    "## Print results\n",
    "print(\"\\nb1. Smallest binary cross-entropy achieved:\\n\\t\"+str(best.loss))\n",
    "print(\"\\nb2. Associated weights:\")\n",
    "for i in range(best.beta.shape[0]):\n",
    "    print(\"\\tb\"+str(i)+\"=\"+str(best.beta[i][0]))\n",
    "\n",
    "## Print corresponding beta to unstandardized x\n",
    "beta_unstd=np.copy(best.beta)\n",
    "for i in range(x.shape[1]):    \n",
    "    beta_unstd[0,0]=beta_unstd[0,0]-np.mean(x_[:,i+1])*beta_unstd[i+1,0]/np.std(x_[:,i+1])\n",
    "    beta_unstd[i+1,0]=beta_unstd[i+1,0]/np.std(x_[:,i+1])\n",
    "print(\"\\nb3. Corresponding beta for non-standardized features:\")\n",
    "for i in range(beta_unstd.shape[0]):\n",
    "    print(\"\\tb\"+str(i)+\"=\"+str(beta_unstd[i][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444f1c3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
